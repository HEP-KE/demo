{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmological Power Spectrum Analysis with Multi-Agent System\n",
    "\n",
    "**Architecture:**\n",
    "- **Data Agent**: Loads observational data\n",
    "- **Modeling Agent**: Handles cosmology models and P(k) computations  \n",
    "- **Viz Agent**: Creates visualizations and comparisons\n",
    "- **Orchestrator Agent**: Coordinates all agents to complete user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install smolagents if needed\n",
    "# !pip install 'smolagents[toolkit]' litellm --upgrade -q\n",
    "#  !pip install classy  # For CLASS cosmology\n",
    "\n",
    "# Set your Gemini API key (get from: https://aistudio.google.com/app/apikey)\n",
    "# export GEMINI_API_KEY=\"your-api-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from smolagents import CodeAgent, tool\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "These wrappers simply expose existing functions as smolagent tools. **All original functions in `codes/` remain completely unmodified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tools\n",
    "Wrapper for: codes.data.load_observational_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def load_observational_data(filepath: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load observational data from text file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the data file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (k, P(k), error) arrays or (None, None, None) if loading fails\n",
    "    \"\"\"\n",
    "    from codes.data import load_observational_data as load_obs_data\n",
    "    return load_obs_data(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosmology Model Tools\n",
    "Wrappers for: codes.cosmology_models.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def LCDM() -> dict:\n",
    "    \"\"\"\n",
    "    Flat ΛCDM baseline (cold dark matter + cosmological constant).\n",
    "\n",
    "    The standard 6-parameter cosmological model with cold dark matter and\n",
    "    cosmological constant dark energy (w = -1).\n",
    "\n",
    "    Papers:\n",
    "        - CLASS code: https://arxiv.org/abs/1104.2933\n",
    "        - Planck 2018 params: https://arxiv.org/abs/1807.06209\n",
    "    \"\"\"\n",
    "    from codes.cosmology_models import LCDM as LCDM_model\n",
    "    return LCDM_model()\n",
    "\n",
    "@tool\n",
    "def nu_mass(sum_mnu_eV: float = 0.10, N_species: int = 1) -> dict:\n",
    "    \"\"\"\n",
    "    ΛCDM + massive neutrinos.\n",
    "\n",
    "    Adds massive neutrinos implemented as non-cold dark matter (ncdm) species.\n",
    "    Massive neutrinos suppress small-scale power via free-streaming. The total\n",
    "    neutrino mass is split equally among N_species degenerate mass eigenstates.\n",
    "\n",
    "    Args:\n",
    "        sum_mnu_eV: Total neutrino mass in eV (default: 0.1 eV)\n",
    "        N_species: Number of massive neutrino species (default: 1)\n",
    "\n",
    "    Papers:\n",
    "        - Lesgourgues & Pastor review: https://arxiv.org/abs/1212.6154\n",
    "        - Planck 2018 neutrino constraints: https://arxiv.org/abs/1807.06209\n",
    "    \"\"\"\n",
    "    from codes.cosmology_models import nu_mass as nu_mass_model\n",
    "    return nu_mass_model(sum_mnu_eV, N_species)\n",
    "\n",
    "@tool\n",
    "def wCDM(w0: float = -0.9) -> dict:\n",
    "    \"\"\"\n",
    "    Dark energy with constant equation of state parameter w0.\n",
    "\n",
    "    Constant dark energy equation of state w = w0 (here w0 ≈ -0.9).\n",
    "    This alters late-time growth and distance relations compared to ΛCDM (w = -1).\n",
    "\n",
    "    Args:\n",
    "        w0: Dark energy equation of state (default: -0.9)\n",
    "\n",
    "    Papers:\n",
    "        - Chevallier-Polarski parametrization: https://arxiv.org/abs/gr-qc/0009008\n",
    "        - Linder review: https://arxiv.org/abs/astro-ph/0208512\n",
    "\n",
    "    Note: Returns a dict with special '_w0_approx' key for post-processing if CLASS\n",
    "          doesn't support fluid dark energy.\n",
    "    \"\"\"\n",
    "    from codes.cosmology_models import wCDM as wCDM_model\n",
    "    return wCDM_model(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Tools\n",
    "Wrappers for: codes.analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def compute_power_spectrum(params: dict, k_values: object) -> object:\n",
    "    \"\"\"\n",
    "    Compute power spectrum for given cosmological parameters.\n",
    "\n",
    "    Args:\n",
    "        params: Dictionary of cosmological parameters\n",
    "        k_values: Array of k values to compute P(k)\n",
    "\n",
    "    Returns:\n",
    "        Array of P(k) values or None if computation fails\n",
    "    \"\"\"\n",
    "    from codes.analysis import compute_power_spectrum as compute_pk\n",
    "    return compute_pk(params, k_values)\n",
    "\n",
    "@tool\n",
    "def compute_all_models(k_values: object, models: dict = None) -> dict:\n",
    "    \"\"\"\n",
    "    Compute power spectra for all defined models.\n",
    "    \n",
    "    Args:\n",
    "        k_values: Array of k values in h/Mpc\n",
    "        models: Optional dictionary of models. If None, uses define_cosmology_models()\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model names as keys and P(k) arrays as values\n",
    "    \"\"\"\n",
    "    from codes.analysis import compute_all_models as compute_all\n",
    "    return compute_all(k_values, models)\n",
    "\n",
    "@tool\n",
    "def compute_suppression_ratios(model_results: dict, k_values: object, reference_model: str = 'ΛCDM') -> dict:\n",
    "    \"\"\"\n",
    "    Compute power spectrum suppression relative to a reference model.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary with model names as keys and P(k) arrays as values\n",
    "        k_values: Array of k values\n",
    "        reference_model: Name of the reference model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with model names as keys and suppression ratios as values\n",
    "    \"\"\"\n",
    "    from codes.analysis import compute_suppression_ratios as compute_suppression\n",
    "    return compute_suppression(model_results, k_values, reference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Tools\n",
    "Wrappers for: codes.viz.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def plot_power_spectra(k_theory: object, model_results: dict, k_obs: object, Pk_obs: object, σPk_obs: object, save_path: str = 'plots/power_spectra_agent.png') -> str:\n",
    "    \"\"\"\n",
    "    Create plot comparing theoretical models with observations.\n",
    "\n",
    "    Args:\n",
    "        k_theory: k values for theoretical models\n",
    "        model_results: Dictionary with model names and P(k) arrays\n",
    "        k_obs: k values for observations\n",
    "        Pk_obs: P(k) values for observations\n",
    "        σPk_obs: Errors on P(k) observations\n",
    "        save_path: Path to save the figure (default: plots/power_spectra_agent.png)\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved plot\n",
    "    \"\"\"\n",
    "    from codes.viz import plot_power_spectra as plot_pk\n",
    "    return plot_pk(k_theory, model_results, k_obs, Pk_obs, σPk_obs, save_path)\n",
    "\n",
    "@tool\n",
    "def plot_suppression_ratios(k_values: object, suppression_ratios: dict, reference_model: str = 'ΛCDM', save_path: str = 'plots/suppression_ratios_agent.png') -> str:\n",
    "    \"\"\"\n",
    "    Plot power spectrum suppression relative to reference model.\n",
    "    \n",
    "    Args:\n",
    "        k_values: Array of k values\n",
    "        suppression_ratios: Dictionary with model names and suppression arrays\n",
    "        reference_model: Name of the reference model\n",
    "        save_path: Path to save the figure (default: plots/suppression_ratios_agent.png)\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved plot\n",
    "    \"\"\"\n",
    "    from codes.viz import plot_suppression_ratios as plot_suppression\n",
    "    return plot_suppression(k_values, suppression_ratios, reference_model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Google Gemini 2.5 Flash (recommended)\n",
    "from smolagents import LiteLLMModel\n",
    "import os\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"gemini/gemini-2.5-flash\",\n",
    "    api_key=\"AIzaSyAadmk8x3NVWogtiwVOCtLnC0Xfkzoma8Q\"\n",
    "    # api_key=os.getenv(\"GEMINI_API_KEY\")  # Set via: export GEMINI_API_KEY=\"your-key\"\n",
    ")\n",
    "\n",
    "# Option 2: Use Hugging Face Inference API\n",
    "# from smolagents import InferenceClientModel\n",
    "# model = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\", timeout=120)\n",
    "\n",
    "# Option 3: Use OpenAI\n",
    "# from smolagents import OpenAIModel\n",
    "# model = OpenAIModel(model_id=\"gpt-4\")\n",
    "\n",
    "# Option 4: Use Anthropic Claude\n",
    "# from smolagents import AnthropicModel\n",
    "# model = AnthropicModel(model_id=\"claude-3-5-sonnet-20241022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Agent: Loads observational data\n",
    "data_agent = CodeAgent(\n",
    "    tools=[load_observational_data],\n",
    "    model=model,\n",
    "    max_steps=5,\n",
    "    verbosity_level=2,  # 0=minimal, 1=normal, 2=detailed (shows agent's code & thinking)\n",
    "    additional_authorized_imports=[\"numpy\", \"matplotlib\"],\n",
    "    name=\"data_agent\",\n",
    "    description=\"Loads observational data from eBOSS DR14 Lyman-alpha forest.\"\n",
    ")\n",
    "\n",
    "# Modeling Agent: Handles cosmology models and P(k) computations\n",
    "modeling_agent = CodeAgent(\n",
    "    tools=[LCDM, nu_mass, wCDM, compute_power_spectrum, compute_all_models, compute_suppression_ratios],\n",
    "    model=model,\n",
    "    max_steps=15,\n",
    "    verbosity_level=2,\n",
    "    additional_authorized_imports=[\"numpy\", \"matplotlib\"],\n",
    "    name=\"modeling_agent\",\n",
    "    description=\"Computes linear P(k) predictions for ΛCDM, massive neutrinos, and wCDM models.\"\n",
    ")\n",
    "\n",
    "# Viz Agent: Creates visualizations\n",
    "viz_agent = CodeAgent(\n",
    "    tools=[plot_power_spectra, plot_suppression_ratios],\n",
    "    model=model,\n",
    "    max_steps=5,\n",
    "    verbosity_level=2,\n",
    "    additional_authorized_imports=[\"numpy\", \"matplotlib\"],\n",
    "    name=\"viz_agent\",\n",
    "    description=\"Creates visualizations comparing theoretical P(k) predictions with observations.\"\n",
    ")\n",
    "\n",
    "# Orchestrator: Coordinates all agents\n",
    "orchestrator = CodeAgent(\n",
    "    tools=[],\n",
    "    model=model,\n",
    "    managed_agents=[data_agent, modeling_agent, viz_agent],\n",
    "    max_steps=20,\n",
    "    verbosity_level=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is the captial of France?</span>                                                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - gemini/gemini-2.5-flash ────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is the captial of France?\u001b[0m                                                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - gemini/gemini-2.5-flash \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">litellm.AuthenticationError: geminiException - {</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">  </span><span style=\"color: #008000; text-decoration-color: #008000\">\"error\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: {</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">\"code\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">\"message\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"API key not valid. Please pass a valid API key.\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">\"status\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"INVALID_ARGUMENT\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">\"details\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: [</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">      {</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"@type\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"type.googleapis.com/google.rpc.ErrorInfo\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"reason\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"API_KEY_INVALID\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"domain\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"googleapis.com\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"metadata\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: {</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">          </span><span style=\"color: #008000; text-decoration-color: #008000\">\"service\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"generativelanguage.googleapis.com\"</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        }</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">      },</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">      {</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"@type\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"type.googleapis.com/google.rpc.LocalizedMessage\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"locale\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"en-US\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">,</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">\"message\"</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"API key not valid. Please pass a valid API key.\"</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">      }</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    \\]</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">  }</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;31mlitellm.AuthenticationError: geminiException - \u001b[0m\u001b[1;31m{\u001b[0m\n",
       "\u001b[1;31m  \u001b[0m\u001b[32m\"error\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;31m{\u001b[0m\n",
       "\u001b[1;31m    \u001b[0m\u001b[32m\"code\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m    \u001b[0m\u001b[32m\"message\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"API key not valid. Please pass a valid API key.\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m    \u001b[0m\u001b[32m\"status\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"INVALID_ARGUMENT\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m    \u001b[0m\u001b[32m\"details\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;31m[\u001b[0m\n",
       "\u001b[1;31m      \u001b[0m\u001b[1;31m{\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"@type\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"type.googleapis.com/google.rpc.ErrorInfo\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"reason\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"API_KEY_INVALID\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"domain\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"googleapis.com\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"metadata\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[1;31m{\u001b[0m\n",
       "\u001b[1;31m          \u001b[0m\u001b[32m\"service\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"generativelanguage.googleapis.com\"\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[1;31m}\u001b[0m\n",
       "\u001b[1;31m      \u001b[0m\u001b[1;31m}\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m      \u001b[0m\u001b[1;31m{\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"@type\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"type.googleapis.com/google.rpc.LocalizedMessage\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"locale\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"en-US\"\u001b[0m\u001b[1;31m,\u001b[0m\n",
       "\u001b[1;31m        \u001b[0m\u001b[32m\"message\"\u001b[0m\u001b[1;31m: \u001b[0m\u001b[32m\"API key not valid. Please pass a valid API key.\"\u001b[0m\n",
       "\u001b[1;31m      \u001b[0m\u001b[1;31m}\u001b[0m\n",
       "\u001b[1;31m    \\\u001b[0m\u001b[1;31m]\u001b[0m\n",
       "\u001b[1;31m  \u001b[0m\u001b[1;31m}\u001b[0m\n",
       "\u001b[1;31m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.20 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.20 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AgentGenerationError",
     "evalue": "Error in generating model output:\nlitellm.AuthenticationError: geminiException - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"API key not valid. Please pass a valid API key.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_INVALID\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"generativelanguage.googleapis.com\"\n        }\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.LocalizedMessage\",\n        \"locale\": \"en-US\",\n        \"message\": \"API key not valid. Please pass a valid API key.\"\n      }\n    ]\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1537\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39murl, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:576\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:558\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    557\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m--> 558\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '400 Bad Request' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=None'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mVertexAIError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/main.py:2426\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m deepcopy(optional_params)\n\u001b[0;32m-> 2426\u001b[0m     response \u001b[38;5;241m=\u001b[39m vertex_chat_completion\u001b[38;5;241m.\u001b[39mcompletion(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2427\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2428\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   2429\u001b[0m         model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[1;32m   2430\u001b[0m         print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[1;32m   2431\u001b[0m         optional_params\u001b[38;5;241m=\u001b[39mnew_params,\n\u001b[1;32m   2432\u001b[0m         litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2433\u001b[0m         logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[1;32m   2434\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   2435\u001b[0m         vertex_location\u001b[38;5;241m=\u001b[39mvertex_ai_location,\n\u001b[1;32m   2436\u001b[0m         vertex_project\u001b[38;5;241m=\u001b[39mvertex_ai_project,\n\u001b[1;32m   2437\u001b[0m         vertex_credentials\u001b[38;5;241m=\u001b[39mvertex_credentials,\n\u001b[1;32m   2438\u001b[0m         gemini_api_key\u001b[38;5;241m=\u001b[39mgemini_api_key,\n\u001b[1;32m   2439\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[1;32m   2440\u001b[0m         acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[1;32m   2441\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   2442\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   2443\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m   2444\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   2445\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1541\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[1;32m   1542\u001b[0m         status_code\u001b[38;5;241m=\u001b[39merror_code,\n\u001b[1;32m   1543\u001b[0m         message\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   1544\u001b[0m         headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1545\u001b[0m     )\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n",
      "\u001b[0;31mVertexAIError\u001b[0m: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"API key not valid. Please pass a valid API key.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_INVALID\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"generativelanguage.googleapis.com\"\n        }\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.LocalizedMessage\",\n        \"locale\": \"en-US\",\n        \"message\": \"API key not valid. Please pass a valid API key.\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/agents.py:1638\u001b[0m, in \u001b[0;36mCodeAgent._step_stream\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m     chat_message: ChatMessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1639\u001b[0m         input_messages,\n\u001b[1;32m   1640\u001b[0m         stop_sequences\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<end_code>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling tools:\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_args,\n\u001b[1;32m   1642\u001b[0m     )\n\u001b[1;32m   1643\u001b[0m     memory_step\u001b[38;5;241m.\u001b[39mmodel_output_message \u001b[38;5;241m=\u001b[39m chat_message\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/models.py:1130\u001b[0m, in \u001b[0;36mLiteLLMModel.generate\u001b[0;34m(self, messages, stop_sequences, response_format, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m completion_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_completion_kwargs(\n\u001b[1;32m   1118\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1119\u001b[0m     stop_sequences\u001b[38;5;241m=\u001b[39mstop_sequences,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1128\u001b[0m )\n\u001b[0;32m-> 1130\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompletion_kwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_input_token_count \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mprompt_tokens\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/utils.py:1279\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1277\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1278\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1279\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/utils.py:1157\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1158\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/main.py:3220\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3219\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   3221\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   3222\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   3223\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   3224\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   3225\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   3226\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2232\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1157\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(\n\u001b[1;32m   1158\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1159\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1160\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   1161\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m   1162\u001b[0m     )\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m403\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: litellm.AuthenticationError: geminiException - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"API key not valid. Please pass a valid API key.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_INVALID\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"generativelanguage.googleapis.com\"\n        }\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.LocalizedMessage\",\n        \"locale\": \"en-US\",\n        \"message\": \"API key not valid. Please pass a valid API key.\"\n      }\n    ]\n  }\n}\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_query\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the captial of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m orchestrator\u001b[38;5;241m.\u001b[39mrun(test_query)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/agents.py:442\u001b[0m, in \u001b[0;36mMultiStepAgent.run\u001b[0;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[1;32m    439\u001b[0m run_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Outputs are returned only at the end. We only look at the last step.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stream(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask, max_steps\u001b[38;5;241m=\u001b[39mmax_steps, images\u001b[38;5;241m=\u001b[39mimages))\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(steps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], FinalAnswerStep)\n\u001b[1;32m    444\u001b[0m output \u001b[38;5;241m=\u001b[39m steps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/agents.py:530\u001b[0m, in \u001b[0;36mMultiStepAgent._run_stream\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    527\u001b[0m             final_answer \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m     action_step\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/agents.py:517\u001b[0m, in \u001b[0;36mMultiStepAgent._run_stream\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_rule(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mLogLevel\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_stream(action_step):\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;66;03m# Yield streaming deltas\u001b[39;00m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, (ActionOutput, ToolOutput)):\n\u001b[1;32m    520\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/smolagents/agents.py:1660\u001b[0m, in \u001b[0;36mCodeAgent._step_stream\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1658\u001b[0m     memory_step\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m output_text\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AgentGenerationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in generating model output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;66;03m### Parse output ###\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAgentGenerationError\u001b[0m: Error in generating model output:\nlitellm.AuthenticationError: geminiException - {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"API key not valid. Please pass a valid API key.\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n        \"reason\": \"API_KEY_INVALID\",\n        \"domain\": \"googleapis.com\",\n        \"metadata\": {\n          \"service\": \"generativelanguage.googleapis.com\"\n        }\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.LocalizedMessage\",\n        \"locale\": \"en-US\",\n        \"message\": \"API key not valid. Please pass a valid API key.\"\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "test_query= \"What is the captial of France?\"\n",
    "result = orchestrator.run(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Using the observational data from eBOSS DR14 Lyman-alpha forest (data/DR14_pm3d_19kbins.txt), \n",
    "compare the linear P(k) values for ΛCDM, ΛCDM with massive neutrinos (Σmν=0.10 eV), and dark \n",
    "energy model with equation of state parameter w0=-0.9. \n",
    "\n",
    "Create visualizations showing:\n",
    "1. The power spectra comparison with observational data\n",
    "2. The suppression ratios relative to ΛCDM\n",
    "\n",
    "Comment on how close the P(k) values are and analyze the power spectrum suppression compared to ΛCDM.\n",
    "\"\"\"\n",
    "\n",
    "result = orchestrator.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Agent-Created Plots\n",
    "\n",
    "The agents saved plots to `plots/` directory. Display them here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Agent Steps (Optional)\n",
    "\n",
    "See detailed step-by-step breakdown of what the agent did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Display plots created by agents\n",
    "plot_files = [\n",
    "    'plots/power_spectra_agent.png',\n",
    "    'plots/suppression_ratios_agent.png'\n",
    "]\n",
    "\n",
    "for plot_file in plot_files:\n",
    "    if os.path.exists(plot_file):\n",
    "        print(f\"\\n{plot_file}\")\n",
    "        display(Image(filename=plot_file))\n",
    "    else:\n",
    "        print(f\"⚠️  {plot_file} not found - agent may not have created this plot yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the agent's logs to see what it did\n",
    "print(\"\\nAgent Steps\")\n",
    "for i, log_entry in enumerate(orchestrator.logs, 1):\n",
    "    print(f\"\\nStep {i}:\")\n",
    "    print(f\"  Task: {log_entry.get('task', 'N/A')}\")\n",
    "    if 'agent_name' in log_entry:\n",
    "        print(f\"  Agent: {log_entry['agent_name']}\")\n",
    "    if 'tool_calls' in log_entry:\n",
    "        print(f\"  Tools used: {log_entry['tool_calls']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Agent's Work\n",
    "\n",
    "After the agent finishes, you can review what it did step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Direct Agent Usage\n",
    "\n",
    "You can also use individual agents directly for specific tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use modeling agent directly\n",
    "result = modeling_agent.run(\"\"\"\n",
    "Create a k array from 0.0001 to 10 h/Mpc with 300 points.\n",
    "Compute P(k) for ΛCDM, ΛCDM with massive neutrinos (0.10 eV), and wCDM (w0=-0.9).\n",
    "Calculate the suppression at k=1 h/Mpc for the massive neutrino model relative to ΛCDM.\n",
    "\"\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual workflow\n",
    "manual_agent = CodeAgent(\n",
    "    tools=[\n",
    "        load_observational_data, \n",
    "        LCDM, nu_mass, wCDM, \n",
    "        compute_power_spectrum, compute_all_models, compute_suppression_ratios,\n",
    "        plot_power_spectra, plot_suppression_ratios\n",
    "    ],\n",
    "    model=model,\n",
    "    additional_authorized_imports=[\"numpy\", \"matplotlib\"]\n",
    ")\n",
    "\n",
    "result = manual_agent.run(\"\"\"\n",
    "Load the data, compute the three models (ΛCDM, massive neutrino with 0.10 eV, wCDM with w0=-0.9), \n",
    "and create both plots. Tell me the k range of the observational data.\n",
    "\"\"\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Queries\n",
    "\n",
    "Try your own queries below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom query example\n",
    "custom_query = \"\"\"\n",
    "Calculate the power spectra for the three models and create a comparison plot.\n",
    "Focus on the wavenumber range relevant to Lyman-alpha forest observations (k ~ 0.2 to 2.5 h/Mpc).\n",
    "Tell me the P(k) suppression at k=1 h/Mpc for the massive neutrino and wCDM models.\n",
    "\"\"\"\n",
    "\n",
    "custom_result = orchestrator.run(custom_query)\n",
    "print(custom_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
